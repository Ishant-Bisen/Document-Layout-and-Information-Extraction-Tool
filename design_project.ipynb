{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ishant-Bisen/Document-Layout-and-Information-Extraction-Tool/blob/main/design_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! rm -r transformers\n",
        "! git clone https://github.com/huggingface/transformers.git\n",
        "! cd transformers\n",
        "! pip install ./transformers"
      ],
      "metadata": {
        "id": "l2JYgBI_otSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://guillaumejaume.github.io/FUNSD/dataset.zip\n",
        "! unzip dataset.zip && mv dataset data && rm -rf dataset.zip __MACOSX"
      ],
      "metadata": {
        "id": "O2ptOIsBoyMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "image = Image.open(\"/content/data/training_data/images/0000971160.png\")\n",
        "image = image.convert(\"RGB\")\n",
        "image"
      ],
      "metadata": {
        "id": "ZSf3jiVao2FS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('/content/data/training_data/annotations/0000971160.json') as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "for annotation in data['form']:\n",
        "  print(annotation)"
      ],
      "metadata": {
        "id": "ur1EUUe0p2AR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "draw = ImageDraw.Draw(image, \"RGBA\")\n",
        "\n",
        "font = ImageFont.load_default()\n",
        "\n",
        "label2color = {'question':'blue', 'answer':'green', 'header':'orange', 'name' : 'red' , 'other':'violet' , 'date' : 'yellow'}\n",
        "\n",
        "for annotation in data['form']:\n",
        "  label = annotation['label']\n",
        "  general_box = annotation['box']\n",
        "  draw.rectangle(general_box, outline=label2color[label], width=2)\n",
        "  draw.text((general_box[0] + 10, general_box[1] - 10), label, fill=label2color[label], font=font)\n",
        "  words = annotation['words']\n",
        "  for word in words:\n",
        "    box = word['box']\n",
        "    draw.rectangle(box, outline=label2color[label], width=1)\n",
        "\n",
        "image"
      ],
      "metadata": {
        "id": "ZerWU2ZkqWlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! python unilm/layoutlm/examples/seq_labeling/preprocess.py --data_dir data/training_data/annotations \\\n",
        "                                                      --data_split train \\\n",
        "                                                      --output_dir data \\\n",
        "                                                      --model_name_or_path microsoft/layoutlm-base-uncased \\\n",
        "                                                      --max_len 510\n",
        "\n",
        "! python unilm/layoutlm/examples/seq_labeling/preprocess.py --data_dir data/testing_data/annotations \\\n",
        "                                                      --data_split test \\\n",
        "                                                      --output_dir data \\\n",
        "                                                      --model_name_or_path microsoft/layoutlm-base-uncased \\\n",
        "                                                      --max_len 510"
      ],
      "metadata": {
        "id": "ENLrYArcqyV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cat data/train.txt | cut -d$'\\t' -f 2 | grep -v \"^$\"| sort | uniq > data/labels.txt"
      ],
      "metadata": {
        "id": "Z6v1clxP7pPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "def get_labels(path):\n",
        "    with open(path, \"r\") as f:\n",
        "        labels = f.read().splitlines()\n",
        "    if \"O\" not in labels:\n",
        "        labels = [\"O\"] + labels\n",
        "    return labels\n",
        "\n",
        "labels = get_labels(\"/content/data/labels.txt\")\n",
        "num_labels = len(labels)\n",
        "label_map = {i: label for i, label in enumerate(labels)}\n",
        "# Use cross entropy ignore index as padding label id so that only real label ids contribute to the loss later\n",
        "pad_token_label_id = CrossEntropyLoss().ignore_index"
      ],
      "metadata": {
        "id": "xW_OqIk7rD5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(labels)"
      ],
      "metadata": {
        "id": "hcT9AZATtgjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "7vUyLfI0zbvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LayoutLMTokenizer\n",
        "from layoutlm.data.funsd import FunsdDataset, InputFeatures\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "args = {'local_rank': -1,\n",
        "        'overwrite_cache': True,\n",
        "        'data_dir': '/content/data',\n",
        "        'model_name_or_path':'microsoft/layoutlm-base-uncased',\n",
        "        'max_seq_length': 512,\n",
        "        'model_type': 'layoutlm',}\n",
        "\n",
        "# class to turn the keys of a dict into attributes (thanks Stackoverflow)\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "args = AttrDict(args)\n",
        "\n",
        "tokenizer = LayoutLMTokenizer.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n",
        "\n",
        "# the LayoutLM authors already defined a specific FunsdDataset, so we are going to use this here\n",
        "train_dataset = FunsdDataset(args, tokenizer, labels, pad_token_label_id, mode=\"train\")\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_dataloader = DataLoader(train_dataset,\n",
        "                              sampler=train_sampler,\n",
        "                              batch_size=2)\n",
        "\n",
        "eval_dataset = FunsdDataset(args, tokenizer, labels, pad_token_label_id, mode=\"test\")\n",
        "eval_sampler = SequentialSampler(eval_dataset)\n",
        "eval_dataloader = DataLoader(eval_dataset,\n",
        "                             sampler=eval_sampler,\n",
        "                            batch_size=2)\n"
      ],
      "metadata": {
        "id": "gqdcP_SwwHxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataloader)"
      ],
      "metadata": {
        "id": "RoWXSKYOwQb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(eval_dataloader)"
      ],
      "metadata": {
        "id": "kuu5urcLwV-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "input_ids = batch[0][0]\n",
        "tokenizer.decode(input_ids)"
      ],
      "metadata": {
        "id": "sViPWaeT87XK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LayoutLMForTokenClassification\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = LayoutLMForTokenClassification.from_pretrained(\"microsoft/layoutlm-base-uncased\", num_labels=num_labels)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "PyWnRLtX9KWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "metadata": {
        "id": "nEdf0bQmSoPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "global_step = 0\n",
        "num_train_epochs = 10\n",
        "t_total = len(train_dataloader) * num_train_epochs # total number of training steps \n",
        "\n",
        "#put the model in training mode\n",
        "model.train()\n",
        "for epoch in range(num_train_epochs):\n",
        "  for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
        "      input_ids = batch[0].to(device)\n",
        "      bbox = batch[4].to(device)\n",
        "      attention_mask = batch[1].to(device)\n",
        "      token_type_ids = batch[2].to(device)\n",
        "      labels = batch[3].to(device)\n",
        "\n",
        "      # forward pass\n",
        "      outputs = model(input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids,\n",
        "                      labels=labels)\n",
        "      loss = outputs.loss\n",
        "\n",
        "       # print loss every 100 steps\n",
        "      if global_step % 100 == 0:\n",
        "        print(f\"Loss after {global_step} steps: {loss.item()}\")\n",
        "\n",
        "      # backward pass to get the gradients \n",
        "      loss.backward()\n",
        "\n",
        "      #print(\"Gradients on classification head:\")\n",
        "      #print(model.classifier.weight.grad[6,:].sum())\n",
        "\n",
        "      # update\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      global_step += 1"
      ],
      "metadata": {
        "id": "i9chIR7_9epA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from seqeval.metrics import (\n",
        "    classification_report,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        ")\n",
        "\n",
        "eval_loss = 0.0\n",
        "nb_eval_steps = 0\n",
        "preds = None\n",
        "out_label_ids = None\n",
        "\n",
        "# put model in evaluation mode\n",
        "model.eval()\n",
        "for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "    with torch.no_grad():\n",
        "        input_ids = batch[0].to(device)\n",
        "        bbox = batch[4].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        token_type_ids = batch[2].to(device)\n",
        "        labels = batch[3].to(device)\n",
        "\n",
        "        # forward pass\n",
        "        outputs = model(input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids,\n",
        "                        labels=labels)\n",
        "        # get the loss and logits\n",
        "        tmp_eval_loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        eval_loss += tmp_eval_loss.item()\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "        # compute the predictions\n",
        "        if preds is None:\n",
        "            preds = logits.detach().cpu().numpy()\n",
        "            out_label_ids = labels.detach().cpu().numpy()\n",
        "        else:\n",
        "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "            out_label_ids = np.append(\n",
        "                out_label_ids, labels.detach().cpu().numpy(), axis=0\n",
        "            )\n",
        "\n",
        "# compute average evaluation loss\n",
        "eval_loss = eval_loss / nb_eval_steps\n",
        "preds = np.argmax(preds, axis=2)\n",
        "\n",
        "out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n",
        "preds_list = [[] for _ in range(out_label_ids.shape[0])]\n",
        "\n",
        "for i in range(out_label_ids.shape[0]):\n",
        "    for j in range(out_label_ids.shape[1]):\n",
        "        if out_label_ids[i, j] != pad_token_label_id:\n",
        "            out_label_list[i].append(label_map[out_label_ids[i][j]])\n",
        "            preds_list[i].append(label_map[preds[i][j]])\n",
        "\n",
        "results = {\n",
        "    \"loss\": eval_loss,\n",
        "    \"precision\": precision_score(out_label_list, preds_list),\n",
        "    \"recall\": recall_score(out_label_list, preds_list),\n",
        "    \"f1\": f1_score(out_label_list, preds_list),\n",
        "}\n",
        "print(results)"
      ],
      "metadata": {
        "id": "WtU05Thb9wOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install tesseract-ocr\n",
        "!pip install pytesseract"
      ],
      "metadata": {
        "id": "4LgkC--QCu-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytesseract\n",
        "\n",
        "#image = Image.open('/content/form_example.jpg')\n",
        "image = Image.open(\"/content/data/testing_data/images/82253058_3059.png\")\n",
        "image = image.convert(\"RGB\")\n",
        "image"
      ],
      "metadata": {
        "id": "3_hCd6OtC1wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "width, height = image.size\n",
        "w_scale = 1000/width\n",
        "h_scale = 1000/height\n",
        "\n",
        "ocr_df = pytesseract.image_to_data(image, output_type='data.frame') \\\n",
        "            \n",
        "ocr_df = ocr_df.dropna() \\\n",
        "               .assign(left_scaled = ocr_df.left*w_scale,\n",
        "                       width_scaled = ocr_df.width*w_scale,\n",
        "                       top_scaled = ocr_df.top*h_scale,\n",
        "                       height_scaled = ocr_df.height*h_scale,\n",
        "                       right_scaled = lambda x: x.left_scaled + x.width_scaled,\n",
        "                       bottom_scaled = lambda x: x.top_scaled + x.height_scaled)\n",
        "\n",
        "float_cols = ocr_df.select_dtypes('float').columns\n",
        "ocr_df[float_cols] = ocr_df[float_cols].round(0).astype(int)\n",
        "ocr_df = ocr_df.replace(r'^\\s*$', np.nan, regex=True)\n",
        "ocr_df = ocr_df.dropna().reset_index(drop=True)\n",
        "ocr_df[:20]"
      ],
      "metadata": {
        "id": "4cCjvWBxVSK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(ocr_df)"
      ],
      "metadata": {
        "id": "ETmSg_AyVVwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = list(ocr_df.text)\n",
        "coordinates = ocr_df[['left', 'top', 'width', 'height']]\n",
        "actual_boxes = []\n",
        "for idx, row in coordinates.iterrows():\n",
        "  x, y, w, h = tuple(row) # the row comes in (left, top, width, height) format\n",
        "  actual_box = [x, y, x+w, y+h] # we turn it into (left, top, left+widght, top+height) to get the actual box \n",
        "  actual_boxes.append(actual_box)\n",
        "\n",
        "def normalize_box(box, width, height):\n",
        "    return [\n",
        "        int(1000 * (box[0] / width)),\n",
        "        int(1000 * (box[1] / height)),\n",
        "        int(1000 * (box[2] / width)),\n",
        "        int(1000 * (box[3] / height)),\n",
        "    ]\n",
        "\n",
        "boxes = []\n",
        "for box in actual_boxes:\n",
        "  boxes.append(normalize_box(box, width, height))\n",
        "boxes"
      ],
      "metadata": {
        "id": "Y7Jdka41VaEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_example_to_features(image, words, boxes, actual_boxes, tokenizer, args, cls_token_box=[0, 0, 0, 0],\n",
        "                                 sep_token_box=[1000, 1000, 1000, 1000],\n",
        "                                 pad_token_box=[0, 0, 0, 0]):\n",
        "      width, height = image.size\n",
        "\n",
        "      tokens = []\n",
        "      token_boxes = []\n",
        "      actual_bboxes = [] # we use an extra b because actual_boxes is already used\n",
        "      token_actual_boxes = []\n",
        "      for word, box, actual_bbox in zip(words, boxes, actual_boxes):\n",
        "          word_tokens = tokenizer.tokenize(word)\n",
        "          tokens.extend(word_tokens)\n",
        "          token_boxes.extend([box] * len(word_tokens))\n",
        "          actual_bboxes.extend([actual_bbox] * len(word_tokens))\n",
        "          token_actual_boxes.extend([actual_bbox] * len(word_tokens))\n",
        "\n",
        "      # Truncation: account for [CLS] and [SEP] with \"- 2\". \n",
        "      special_tokens_count = 2 \n",
        "      if len(tokens) > args.max_seq_length - special_tokens_count:\n",
        "          tokens = tokens[: (args.max_seq_length - special_tokens_count)]\n",
        "          token_boxes = token_boxes[: (args.max_seq_length - special_tokens_count)]\n",
        "          actual_bboxes = actual_bboxes[: (args.max_seq_length - special_tokens_count)]\n",
        "          token_actual_boxes = token_actual_boxes[: (args.max_seq_length - special_tokens_count)]\n",
        "\n",
        "      # add [SEP] token, with corresponding token boxes and actual boxes\n",
        "      tokens += [tokenizer.sep_token]\n",
        "      token_boxes += [sep_token_box]\n",
        "      actual_bboxes += [[0, 0, width, height]]\n",
        "      token_actual_boxes += [[0, 0, width, height]]\n",
        "      \n",
        "      segment_ids = [0] * len(tokens)\n",
        "\n",
        "      # next: [CLS] token\n",
        "      tokens = [tokenizer.cls_token] + tokens\n",
        "      token_boxes = [cls_token_box] + token_boxes\n",
        "      actual_bboxes = [[0, 0, width, height]] + actual_bboxes\n",
        "      token_actual_boxes = [[0, 0, width, height]] + token_actual_boxes\n",
        "      segment_ids = [1] + segment_ids\n",
        "\n",
        "      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "      # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "      # tokens are attended to.\n",
        "      input_mask = [1] * len(input_ids)\n",
        "\n",
        "      # Zero-pad up to the sequence length.\n",
        "      padding_length = args.max_seq_length - len(input_ids)\n",
        "      input_ids += [tokenizer.pad_token_id] * padding_length\n",
        "      input_mask += [0] * padding_length\n",
        "      segment_ids += [tokenizer.pad_token_id] * padding_length\n",
        "      token_boxes += [pad_token_box] * padding_length\n",
        "      token_actual_boxes += [pad_token_box] * padding_length\n",
        "\n",
        "      assert len(input_ids) == args.max_seq_length\n",
        "      assert len(input_mask) == args.max_seq_length\n",
        "      assert len(segment_ids) == args.max_seq_length\n",
        "      #assert len(label_ids) == args.max_seq_length\n",
        "      assert len(token_boxes) == args.max_seq_length\n",
        "      assert len(token_actual_boxes) == args.max_seq_length\n",
        "      \n",
        "      return input_ids, input_mask, segment_ids, token_boxes, token_actual_boxes"
      ],
      "metadata": {
        "id": "HKmT7izJVfWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids, input_mask, segment_ids, token_boxes, token_actual_boxes = convert_example_to_features(image=image, words=words, boxes=boxes, actual_boxes=actual_boxes, tokenizer=tokenizer, args=args)"
      ],
      "metadata": {
        "id": "UWw7rseAV1rW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(input_ids)"
      ],
      "metadata": {
        "id": "EMt-XRLzV439"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = torch.tensor(input_ids, device=device).unsqueeze(0)\n",
        "input_ids.shape"
      ],
      "metadata": {
        "id": "1y8j0VYoV8qW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_mask = torch.tensor(input_mask, device=device).unsqueeze(0)\n",
        "attention_mask.shape"
      ],
      "metadata": {
        "id": "3T2a8-MNWALE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_type_ids = torch.tensor(segment_ids, device=device).unsqueeze(0)\n",
        "token_type_ids.shape"
      ],
      "metadata": {
        "id": "tzr7lTuWWC7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbox = torch.tensor(token_boxes, device=device).unsqueeze(0)\n",
        "bbox.shape"
      ],
      "metadata": {
        "id": "c9Q4bw6_WFoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "outputs"
      ],
      "metadata": {
        "id": "y3le75pBWIzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.logits.shape"
      ],
      "metadata": {
        "id": "EZrr6wlKWLfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.logits.argmax(-1)"
      ],
      "metadata": {
        "id": "038_g2VpWOxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_predictions = outputs.logits.argmax(-1).squeeze().tolist() # the predictions are at the token level\n",
        "\n",
        "word_level_predictions = [] # let's turn them into word level predictions\n",
        "final_boxes = []\n",
        "for id, token_pred, box in zip(input_ids.squeeze().tolist(), token_predictions, token_actual_boxes):\n",
        "  if (tokenizer.decode([id]).startswith(\"##\")) or (id in [tokenizer.cls_token_id, \n",
        "                                                           tokenizer.sep_token_id, \n",
        "                                                          tokenizer.pad_token_id]):\n",
        "    # skip prediction + bounding box\n",
        "\n",
        "    continue\n",
        "  else:\n",
        "    word_level_predictions.append(token_pred)\n",
        "    final_boxes.append(box)\n",
        "\n",
        "# for id, prediction in zip(input_ids.squeeze().tolist(), predictions):\n",
        "#   if id != 0:\n",
        "#     print(tokenizer.decode([id]), label_map[prediction])\n",
        "print(word_level_predictions)"
      ],
      "metadata": {
        "id": "XD2ufYyWWR1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(word_level_predictions))"
      ],
      "metadata": {
        "id": "KZY-uNqbWUyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(final_boxes))"
      ],
      "metadata": {
        "id": "8-v19Yu0WXHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "draw = ImageDraw.Draw(image)\n",
        "\n",
        "font = ImageFont.load_default()\n",
        "\n",
        "def iob_to_label(label):\n",
        "  if label != 'O':\n",
        "    return label[2:]\n",
        "  else:\n",
        "    return \"other\"\n",
        "\n",
        "label2color = {'question':'blue', 'answer':'green', 'header':'orange', 'other':'violet'}\n",
        "\n",
        "for prediction, box in zip(word_level_predictions, final_boxes):\n",
        "    predicted_label = iob_to_label(label_map[prediction]).lower()\n",
        "    draw.rectangle(box, outline=label2color[predicted_label])\n",
        "    draw.text((box[0] + 10, box[1] - 10), text=predicted_label, fill=label2color[predicted_label], font=font)\n",
        "\n",
        "image"
      ],
      "metadata": {
        "id": "aGwbMjHqWbC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('/content/data/testing_data/annotations/82253058_3059.json') as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "for annotation in data['form']:\n",
        "  print(annotation)"
      ],
      "metadata": {
        "id": "2u-ifirJWoBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.open(\"/content/data/testing_data/images/82253058_3059.png\")\n",
        "image = image.convert('RGB')\n",
        "\n",
        "draw = ImageDraw.Draw(image)\n",
        "\n",
        "label2color = {'question':'blue', 'answer':'green', 'header':'orange', 'other':'violet'}\n",
        "\n",
        "for annotation in data['form']:\n",
        "  label = annotation['label']\n",
        "  general_box = annotation['box']\n",
        "  draw.rectangle(general_box, outline=label2color[label], width=2)\n",
        "  draw.text((general_box[0] + 10, general_box[1] - 10), label, fill=label2color[label], font=font)\n",
        "  words = annotation['words']\n",
        "  for word in words:\n",
        "    box = word['box']\n",
        "    draw.rectangle(box, outline=label2color[label], width=1)\n",
        "\n",
        "image"
      ],
      "metadata": {
        "id": "0g7xeMVYWrQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "V7uAeGJf5LIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "GaoQA_Z13d9V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}